\documentclass[11pt]{report}

\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usepackage{amsmath}
\usepackage{amsfonts}
\newcommand{\Ep}{\mathbb{E}}
\newcommand{\Real}{\mathcal{R}}

\begin{document}
\title{Development and Derivation of the Psychological Model}
\maketitle

This document provides development logs of the psychological model, including intuitions, derivations, and result summaries.

To begin with, we must outline several key concepts in building the blocks of our models. 
\begin{itemize}
\item $K$ aspects. We assume there are $K$ aspects based on which a user measures the items. The number $K$ is predefined. Consequently. we define the variables, $U,V$ where  $v_{i,k}$ denotes item $i$'s feature on aspect $k$ and $u_{p,k}$  denotes user $p$'s preference on aspect $k$. The range of $U,V$ is specified by the model, thus a probabilistic generative model and a numerical model might provide apply different range constraints.
\item Prominent aspect(s). We assume that the $K$ aspects are not treated equally by the users. One or some (depending on the model) of the aspects are most important. 
\item Distinguish prominent and non-prominent aspects. We adopt different scoring or ranking methods for prominent and non-prominent aspects. This part essentially makes our model non-compensate.   
\end{itemize}

\part{Ranking Based Models}
This part devotes to models that observe rankings. In each session, we assume that the user is given a set of items, and the user generates a set of pairwise rankings. For example, if a user has access to items $i,j$, and he buys $i$, clicks on $j$. Then we can make a reasonable conclusion that to the user $i\succ j$. The ranking based models mimic the generation of pairwise rankings. The details of observing rankings, i.e. which actions to be taken into accounts and how to grade these actions, are out of the scope of this document (but should be considered in the experiments).

\section{The BTL Model}
\subsection{Model}
In the extended BTL model, a ranking $i \succ j$ is likely to happen if the latent utility score $i$ is relevantly larger than $j$, $p(i \succ j)=\frac{i}{i+ \theta j}$.  The parameter $\theta>1$ plays as a tolerance threshold. If the absolute difference between $i$ and $j$ is not significant $|i-j|\leq \theta$, the user will consider it to be a tie $i=j$, $p(i = j)=\frac{(\theta^2-1)i j}{[i+\theta j][\theta i+ j]}$. Note that $p(i \succ j)+ p(i \prec j) + p(i=j) =1$. 
 
Based on the extended BTL model, we propose our first attempt of the psychological model. The highlights are (1) we consider only one prominent aspect in each session within the $K$ aspects. (2) The winning item must be significantly better than the losing item on the prominent aspect. (3) The winning item can be not as good as the losing product as long as $\theta v_{i,k} >  v_{j,k}$. The usage of parameter $\theta$ can be interpreted as the consumer sets a minimally acceptable level of performance. The cutoff point is controlled by both $\theta$ and how well other items perform.

Therefore the probability of  $w$ winning in the session $d$ is defined in Equ.~\ref{equ:win} as the product of the probability that $w$ outranks other items $l$ on the most pertinent aspect $p(w_k\succ l_k)$ and the probability that $w$ at least ties with other items on other aspects $p(w_{k'} \succeq l_{k'})$.
\begin{equation}\label{equ:win}
p(<w,l>|\Theta,g)  =  \Pi_{k=1}^{K}[ {\frac{w_k}{w_k+\theta l_k}}^{g_k} { \frac{\theta w_{k}}{l_{k}+\theta w_{k}}}^{1-g_k}]
\end{equation}

Thus we present the likelihood of purchase model under Non-Compensatory Rules (NCR) as follows. The model parameters are denoted as $\Theta=\{\theta,v\in V, u\in U)\}$.     
\begin{equation}\label{equ:likelihood}
		p(D|\Theta)=\Pi_{d\in D} \Sigma_{g} \{\Pi_{w\in W^d, v\in L^d} p(<w,l>|g,\Theta) p(g|u)\}
\end{equation}

The inference is implemented by EM. As in~\cite{Hunter2004MM}, in the M-step of EM, instead of maximizing $Q(\Theta)=E_g \ln p(D,G|\Theta) $ , we compute a minorization function of $Q$ . For the limited space, we skip the derivation here and present the updates of NCR in Equ.~\ref{equ:update}. 
\begin{align}\label{equ:update}
\gamma(d,k,\Theta^t) =&\frac{u_k \Pi_{w \in W^d, v\in L^d} \frac{w_k}{w_k+\theta^t v_k}\Pi_{k'\neq k}\frac{\theta^t w_{k'}}{v_{k'}+\theta^t w_{k'}}}{\Sigma_{k=1}^K u_k \Pi_{w \in W^d, v\in L^d} \frac{w_k}{w_k+\theta^t v_k}\Pi_{k'\neq k}\frac{\theta^t w_{k'}}{v_{k'}+\theta^t w_{k'}}}\\\nonumber
\alpha(v,v',k,\Theta^t)=&v_k^t + \theta^t {v'}_k^t\\\nonumber
u_k = & \frac{\Sigma_{u(d)=u}\gamma(d,k,\Theta^t)}{\Sigma_{s=1}^K \Sigma_{u(d)=u}\gamma(d,s,\Theta^t)} \\\nonumber
\frac{1}{v_k}= &\frac{\Sigma_{d\in W(v)}\Sigma_{v'\in L_d} [\frac{\gamma(d,k,\Theta^t)}{ \alpha(v,v',k,\Theta^t)} +\Sigma_{k'\neq k}\frac{\theta^t\gamma(d,k',\Theta^t)}{\alpha(v',v,k,\Theta^t)}]}{\Sigma_{d\in W(v)}|L_d|}\\\nonumber
 & + \frac{\Sigma_{d\in L(v)}\Sigma_{v'\in W_d} [\frac{\theta^t \gamma(d,k,\Theta^t)}{\alpha(v',v,k,\Theta^t)}+\Sigma_{k'\neq k} \frac{\gamma(d,k',\Theta^t)}{\alpha(v,v',k,\Theta^t)}] }{\Sigma_{d\in W(v)}|L_d|}\\\nonumber
\theta = & \frac{(K-1)\Sigma_d |W_d| |L_d|}{\Sigma_d \Sigma_k \gamma(d,k,\Theta^t)\Sigma_{w,v} [\frac{v_k}{\alpha(w,v,k,\Theta^t)}+\Sigma_{k'\neq k} \frac{w_{k'}}{\alpha(v,w,k',\Theta^t)}]}
\end{align}

 
%We use the BTL model with ties $p(i\succ j|g_k=1)=\frac{\theta v_i}{\theta v_i+v_j}$. Therefore, on the prominent aspects, the winning product is likely to be better than the losing product, but on the non-prominent aspects, the winning product can be not as good as the losing product as long as $\theta v_i \succ  v_j$. For simplicity, the value of $\theta>1$ is pre-set. Below, we will use $o$ to denote the observations. 

\subsection{Results}
Unfortunately, the BTL model does NOT outperform comparative models, e.g. BPR on our Tmall dataset. But it outranks BPR and other models (excluding classification and regression models) on the Yochoose dataset. 

Nonetheless, we run experiments based on BTL for a series of evaluations. The first one is about different actions. Our conclusions include: (1) Ranking buy v.s. add to cart is slightly better than BPR. However,  ranking solely on other types of actions suffer from the lack of data. (2) Ranking based on ensemble action pairs (3) 

\section{BTR}
We want to make the smallest modification to the BPR model. We use the Bayesian maximum posterior estimator 
\begin{equation}
BPR-Opt = \ln p(i\succ_d j|\Theta) p(\Theta)
\end{equation}

where $\ln p(i\succ_d j|\Theta) =\sigma(x)=\frac{1}{1+\exp{-x_{u,i,j}}}$, $\Theta$ is the set of all parameters. In the first type of BPR (with matrix factorization), $x_{u,i,j}=u^T(i-j)$, $\Theta=U,V$ is the set of all user and item latent vectors. Here, we introduce a new parameter $\theta$ and define:
\begin{equation}
x_{u,i,j}=\sum_k \frac{\exp (u_k)}{\sum_{k'} \exp (u_{k'})}[\exp \theta(i_k-j_k)+\sum_{k'}(i_{k'}-j_{k'})]
\end{equation}


As in ~\cite{Rendle2009BPR},  the gradient of BPR-Opt with respect to the model parameter is

\begin{equation}
\frac{\partial BPR-Opt}{\partial \theta}\propto \sum_{i\succ_d j} \frac{-e^{-x_{u,i,j}}}{1+ e^{-x_{u,i,j}} } \frac{\partial x_{u,i,j}}{\partial \Theta} -\lambda_\Theta \Theta
\end{equation}

where 
\begin{equation}\label{equ:derivative}
 \frac{\partial x_{u,i,j}}{\partial \Theta}=\left\{\begin{matrix}
[\exp\theta (i_k-j_k)+\sum_{k'\neq k}(i_{k'}-j_{k'})][\frac{\exp u_k}{\sum_{k}\exp u_k}-\frac{\exp^2 u_k}{(\sum_k \exp u_k)^2}] & if \Theta=u_k \\ 
\frac{\exp u_k \exp \theta +\sum_{k'\neq k} \exp u_{k'}}{\sum_k \exp u_k}  & if \Theta= i_k \\ 
\frac{-\exp u_k \exp \theta -\sum_{k'\neq k} \exp u_{k'}}{\sum_k \exp u_k}  & if \Theta= i_k \\ 
\sum_k  \frac{\exp (u_k)(i_k-j_k )}{\sum_{k'} \exp (u_{k'})} \exp \theta & if \Theta = \theta 
\end{matrix}\right.
\end{equation}

 \section{AMF}
 
Neighborhood information can also be incorporated in the definition of $x_{u,i,j}$. As in ~\cite{Steck2015Gaussian}, in this paper, we adopt the asymmetric matrix factorization (AMF)~\cite{Steck2015Gaussian}, each item i is associated with two factor vectors $q_i$ and $p_i$. The representation of user $u$ is through the sum $\sum_{j \in R(u)} p_j/\sqrt{|R(u)|}$, where $R(u)$ is the set of positive items.  
 
  \begin{equation}
\hat{r_{ui}}=u+b_u+b_i+q_i (\sum_{s \in R(u)} p_s/\sqrt{|R(u)|} )
\end{equation}

FSBPR~\cite{Zhao2018Factored} incorporates AMF in a pairwise BPR style ranking framework. First, the difference of prediction is defined as :
 
\begin{equation}
x_{u,i,j} = \hat{r_{ui}}- \hat{r_{uj}} = \sum_k [\frac{\sum_{s \in R(u)}  p_s}{\sqrt{|R(u)|}}]_k (q_{i,k}-q_{j,k})
\end{equation}

According to the chain rule $\frac{\partial BPR-Opt}{\partial \theta}\propto \sum_{i\succ_d j} \frac{-e^{-x_{u,i,j}}}{1+ e^{-x_{u,i,j}} } \frac{\partial x_{u,i,j}}{\partial \Theta} -\lambda_\Theta \Theta$, gradient descent is applied to update the parameters $\Theta \leftarrow \Theta - \alpha (\frac{-e^{-x_{u,i,j}}}{1+ e^{-x_{u,i,j}} } \frac{\partial x_{u,i,j}}{\partial \Theta} -\lambda_\Theta \Theta)$, where $\alpha$ is the learning rate, and $ \frac{\partial x_{u,i,j}}{\partial \Theta}$ is:


\begin{equation}\label{equ:BPR}
 \frac{\partial x_{u,i,j}}{\partial \Theta}=\left\{\begin{matrix}
\sum_{u | s\in R(u) }\frac{q_{i_k}-q_{j_k}} {\sqrt{|R(u)|} } & if \Theta=p_{s _k} \\ 
[\frac{\sum_{s \in R(u)}  p_s}{\sqrt{|R(u)|}}]_k   & if \Theta= q_{i_k} \\ 
-[\frac{\sum_{s \in R(u)}  p_s}{\sqrt{|R(u)|}}]_k  & if \Theta= q_{j_k} \\ 
\end{matrix}\right.
\end{equation}

To build the non-compensatory version of AMF, we set $u_k = [\frac{\sum_{s \in R(u)}  p_s}{\sqrt{|R(u)|}}]_k$, and thus we have:

\begin{equation}\label{equ:derivative}
 \frac{\partial x_{u,i,j}}{\partial \Theta}=\left\{\begin{matrix}
\sum_{u | s\in R(u) } [\exp\theta (i_k-j_k)+\sum_{k'\neq k}(i_{k'}-j_{k'})][\frac{\exp u_k}{\sum_{k}\exp u_k}-\frac{\exp^2 u_k}{(\sum_k \exp u_k)^2}] \frac{p_{s_k}}{\sqrt{|R(u)|} }& if \Theta=p_{s_k} \\ 
\frac{\exp u_k \exp \theta +\sum_{k'\neq k} \exp u_{k'}}{\sum_k \exp u_k}  & if \Theta= q_{i_k}  \\ 
\frac{-\exp u_k \exp \theta -\sum_{k'\neq k} \exp u_{k'}}{\sum_k \exp u_k}  & if \Theta= q_{j_k} \\ 
\sum_k  \frac{\exp (u_k)(i_k-j_k )}{\sum_{k'} \exp (u_{k'})} \exp \theta & if \Theta = \theta 
\end{matrix}\right.
\end{equation}

 \section{LCR}
 \textbf{L}ocal \textbf{C}ollaborative \textbf{R}anking (LCR)~\cite{Lee2014Local} minimizes a pair-wise loss function, which is constructed using observed entries. In the experiments~\cite{Lee2014Local},  the log-loss function is shown to achieve best results, thus it is adopted in this paper.
 \begin{equation}
L=\sum_{u\in U} \frac{1}{s_u} \sum_{k=1}^{s_u} \Delta M_{u,i,j}  \log (1+\exp - g(u,i,j) ),
\end{equation}

where $s_u$ is the number of ordered items rated by the user $u$. The loss function approximates the observed rating difference with predicted rating difference $g(u,i,j)$. LCR is inspired by LLORMA, which assumes that the rating matrix itself is not low-rank. Instead, $M$ is locally low-rank where locality is defined by a neighborhood with respect to the given kernel $K$. Therefore, the prediction difference can be approximated by a set of low-rank factorizations around $q$ anchor points $i_s$: $g(u,i,j)=\sum_{t=1}^{q} \sum_k u_{t, k} [\frac{K((u_t,i_t),(u,i))}{\sum_{s=1}{q} K((u_s,i_s),(u,i))} i_{t,k} - \frac{K((u_t,i_t),(u,j))}{\sum_{s=1}^{q} K((u_s,i_s),(u,j))} j_{t,k}] $.  \footnote{This equation is not the same as Equ.15 in the paper}

Again, we can modify the score difference $g(u,i,j) = \sum_{t=1}^{q} \sum_k \frac{\exp (u_{t,k})}{\sum_{k'} \exp (u_{t,k'})}[\exp \theta(s (u,t,i) i_{t,k} -s(u,t,j) j_{t,k})+\sum_{k'}(s(t,u,i) i_{t,k'}-s(t,u,j) j_{t,k'})]$, where $s(t,u,i)= \frac{K((u_t,i_t),(u,i))}{\sum_{s=1}^{q} K((u_s,i_s),(u,i))}, s(t,u,j)= \frac{K((u_t,j_t),(u,j))}{\sum_{s=1}^{q} K((u_s,i_s),(u,j))} $. Applying the chain rule, the gradient is $\frac{\partial L}{\partial \Theta}=\frac{\partial L}{\partial g(u,i,j) } \frac{\partial g (u,i,j)}{\theta}$, which is defined by Equ.~\ref{equ:derivative}. Thus, the update steps will be:

\begin{eqnarray}\label{equ:LCR}
line 12: & s_{t,u,i} & \leftarrow \frac{K(u_t,u) K(i_t,i) }{w_{u,i}}  \\\nonumber
line 15: & g_{u,i,j} & = \sum_{t=1}^{q} \sum_k \frac{\exp (u_{t,k})}{\sum_{k'} \exp (u_{t,k'})}[\exp \theta(s (t,u,i) i_{t,k} -s(t,u,j) j_{t,k})+\sum_{k'}(s(t,u,i) i_{t,k'}-s(t,u,j) j_{t,k'})]\\\nonumber
line 15: & l_{u,i,j} & = \frac{-\Delta M \exp - g_{u,i,j} }{ 1 + \exp - g_{u,i,j}} \\\nonumber
line 23: & [\Delta u]_k & \leftarrow [\Delta u]_k  + [\exp\theta (s(t,u,i) i_{t,k}-s(t,u,j) j_{t,k})+\sum_{k'\neq k}(s(t,u,i) i_{t,k'}-s(t,u,j) j_{t,k'})]\\\nonumber 
& & [\frac{\exp u_{t,k}}{\sum_{k}\exp u_{t,k}}-\frac{\exp^2 u_{t,k}}{(\sum_k \exp u_{t,k})^2}]  l_{u,i,j}\\ \nonumber
line 25: & [\Delta i]_k & \leftarrow [\Delta i]_k + \frac{\exp u_k \exp \theta +\sum_{k'\neq k} \exp u_{k'}}{\sum_k \exp u_k}  s(t,u,i) l_{u,i,j} \\ \nonumber
line 26: & [\Delta i]_k & \leftarrow [\Delta i]_k +  \frac{-\exp u_k \exp \theta -\sum_{k'\neq k} \exp u_{k'}}{\sum_k \exp u_k} s(t,u,i) l_{u,i,j}  \\ \nonumber
line 26+: & [\Delta \theta ] & \leftarrow \sum_k  \frac{[s(u,t,i) i_{t,k}- s(u,t,j) j_{t,k} ]\exp (u_k)}{\sum_{k'} \exp (u_{k'})} \exp \theta  \\\nonumber
line 30+: & \theta & = \theta - v  [\Delta \theta ] \\\nonumber
\end{eqnarray}

\section{The Sigmoidal Model}
As the section title indicates, we propose a set of models that relate pair-wise rankings $i\succ j$ to logit functions. The optimization objective is in a general form of :

\begin{eqnarray}
\min_{u,v} & \sum_{i\succ j} \frac{1}{2} (1-\sum_k u_k \sigma(v_{i,k}-v_{j,k}))^2 - \frac{\lambda}{2} \sum_u \|u\|^2 \\\nonumber
 s.t. & \forall u \forall k, u_k>0\\\nonumber
  & \forall u, \sum_k u_k=1
\end{eqnarray}

There are a few issues needed to be emphasized here. (1) Essentially this is a mixture model so only evaluation on prominent aspects is provided. We could make explicit treatments for prominent and non-prominent aspects in the model by changing the constraints to $\forall u,\forall k, 0<u_k<1$. (2) With the negative regularization term (together with the constraints) we penalize sparse $u$s, thus we allow more prominent aspects to be chosen. (3) We use the sum-of-square error function, because our prediction for a pair $<i,,j>$$f(i j)=\sum_k u_k \sigma(v_{i,k}-v_{j,k})$ falls in the range of $(0,1)$ and $f(i,j)=1-f(j,i)$. So we label $i\succ j$ to be 1. For symmetry, we don't have to label $j\succ i$. (4) We do not normalize $v$ as $\sigma(v_{i,k}, v_{j,k})$ is already normalized.

The optimization algorithm is listed below. In each iteration, we first optimize over $u$ when $v$s are fixed. We rewrite the objective as $\|Ax-b\|^2-\lambda \|x\|^2$ with constraints $C_i^Tx=d_i, \forall i=1,\cdots,U$, where $x\in \Real^{KU}$, $U$ is the number of users, $x_i=u_{i,k}$, $A\in \Real^{D}$, $D$ is the number of observations and $D\simeq SL$, where $S$ is the number of sessions, and $L$ is the average number of pairs in a session, $C_i\in \Real^{KU}$ where for $1\leq i \leq U, iK+1\leq j \leq i(K+1), C_{i,j}=1, \forall j', C_{i,j'}=0$, $b$ is a vector of all ones, $d_i=1$ is a scalar. Therefore we have

\begin{equation}
\begin{bmatrix}
x^* \\
z^*
\end{bmatrix} = \begin{bmatrix}
2A^TA-\lambda & C^T \\
C & 0
\end{bmatrix}^{-1}
\begin{bmatrix}
2A^Tb \\
d
\end{bmatrix}
\end{equation}
Note that we have $S\geq U$ to filter cold start users, and generally $L\geq K-1$, $K>1$, therefore the matrix is invertible. 

Next we optimize $v$ while keeping $u$ fixed. We adopt stochastic gradient.  As shown below, the gradient is the addiction of two terms:

\begin{eqnarray*}
\frac{\delta L}{\delta v_{i,k}}&=&\sum_{i\succ j} (1-\sum_{k'} u_{k'}\sigma(v_{i,k'},v_{j,k'}))\sigma(v_{i,k},v_{j,k})(1-\sigma(v_{i,k},v_{j,k}))\\
&&+ \sum_{w\succ i} (1-\sum_{k'} u'_{k'}\sigma(v_{w,k'},v_{i,k'}))\sigma(v_{w,k},v_{i,k})(\sigma(v_{w,k},v_{i,k})-1)
\end{eqnarray*}

We can sample an item $j$ from $i\succ j$ and another item $w$ from $w\succ i$,  compute $S_1= (1-\sum_{k'} u_{k'}\sigma(v_{i,k'},v_{j,k'}))\sigma(v_{i,k},v_{j,k})(1-\sigma(v_{i,k},v_{j,k}))$ and $(1-\sum_{k'} u'_{k'}\sigma(v_{w,k'},v_{i,k'}))\sigma(v_{w,k},v_{i,k})(\sigma(v_{w,k},v_{i,k})-1)$, then we update 

\begin{equation}
v_{i,k}=v_{i,k}+\eta |i\succ j| S_1 +\eta |w\succ i| S_2,
\end{equation}

where $\eta$ is the step length, $|\cdot|$ is the size of a set.
 
\part{Rating Based Models}
In this section, we develop a series of ranking based models. These models have the following properties. (1) The models are based on rating observations, i.e. we assign a rating to every action. The actions include the old-fashioned preference ratings (in such case, each rating is a standalone session), actions in a session, positive and negative samples, etc. (2) The models are Bayesian generative models that have good explainability. (3) We control the assumptions by adjusting the hyper-parameters in these models.

\section{Matrix Factorization}

In conventional matrix factorization~\cite{Koren2009Matrix}, the predicted rating can be computed as an inner product of user preferences and item features as follows.

\begin{equation}\label{equ:MF}
 \hat{\mathbf{X}}_{u,q}=\sum_{k=1}^{K} \mathbf{q}_k \mathbf{u}_k
\end{equation}

For a regularized loss function $L=\sum_{(u,q)\in D} ( \mathbf{X}_{u,q}- \hat{\mathbf{X}}_{u,q})^2 + \lambda (\sum_q \| \mathbf{q}\| + \sum_u \| \mathbf{u}\|)$
The update process

\begin{eqnarray}
 \mathbf{u}= \mathbf{u} - \alpha \frac{\partial L}{\partial \mathbf{u}} =  \mathbf{u} +  \alpha \sum_{q\in R(u)}[ (\mathbf{X}_{u,q}- \hat{\mathbf{X}}_{u,q})\frac{\partial \hat{\mathbf{X}}_{u,q}}{\partial \mathbf{u}}] - \alpha\lambda \mathbf{u} \\\nonumber
 \mathbf{q} =  \mathbf{q} - \alpha \frac{\partial L}{\partial \mathbf{q}}=  \mathbf{q} +  \alpha \sum_{u\in R(q)}[ (\mathbf{X}_{u,q}- \hat{\mathbf{X}}_{u,q})\frac{\partial \hat{\mathbf{X}}_{u,q}}{\partial \mathbf{q}}] - \alpha\lambda \mathbf{q} 
\end{eqnarray}



where we can change the prediction to 
\begin{equation}\label{equ:MF-NCR}
 \hat{\mathbf{X}}_{u,q}=\sum_{k=1}^{K} \frac{\exp \mathbf{u}_k}{\sum_{k'} \exp \mathbf{u}_{k'}} [ \exp\theta \mathbf{q}_k  + \sum_{k'\neq k} \mathbf{q}_{k'} ].
\end{equation}

Thus in updating we use
 \begin{equation}\label{equ:derivative}
 \frac{\partial \hat{\mathbf{X}}_{u,q}}{\partial \Theta}=\left\{\begin{matrix}
 [(q_k) \exp\theta +\sum_{k'\neq k}(q_{k'})][\frac{\exp u_k}{\sum_{k}\exp u_k}-\frac{\exp^2 u_k}{(\sum_k \exp u_k)^2}] & if \Theta=u_k \\ 
\frac{\exp u_k \exp \theta +\sum_{k'\neq k} \exp u_{k'}}{\sum_k \exp u_k}  & if \Theta= q_{k}  \\ 
\sum_k  \frac{q_k \exp (u_k)}{\sum_{k'} \exp (u_{k'})} \exp \theta & if \Theta = \theta 
\end{matrix}\right.
\end{equation}

We further consider a model where the user set a cut-off point $b_{u,k}$ for every non-prominent aspect. 
Thus we can change the prediction to 
\begin{equation}\label{equ:MF-NCR}
 \hat{\mathbf{X}}_{u,q}=\sum_{k=1}^{K} \frac{\exp \mathbf{u}_k}{\sum_{k'} \exp \mathbf{u}_{k'}} [ \exp\theta \mathbf{q}_k  + \sum_{k'\neq k} [\mathbf{q}_{k'}-b_{u,k'}]_+ ].
\end{equation}

The interpretation is that (1) the evaluation of $u$ on $q$ is mainly based on the prominent aspect $k$ (2) the user $u$ sets a cut-off point for non prominent aspects $k'$. If the item performance is lower than the threshold $b_{u,k'}$, it does not contribute to the overall evaluation as $[\mathbf{q}_{k'}-b_{u,k'}]_+=0$ if $\mathbf{q}_{k'}-b_{u,k'}\leq 0$

Thus \begin{eqnarray}
 \mathbf{u}= \mathbf{u} - \alpha \frac{\partial L}{\partial \mathbf{u}} =  \mathbf{u} +  \alpha \sum_{q\in R(u)}[ (\mathbf{X}_{u,q}- \hat{\mathbf{X}}_{u,q})\frac{\partial \hat{\mathbf{X}}_{u,q}}{\partial \mathbf{u}}] - \alpha\lambda \mathbf{u} \\\nonumber
 \mathbf{q} =  \mathbf{q} - \alpha \frac{\partial L}{\partial \mathbf{q}}=  \mathbf{q} +  \alpha \sum_{u\in R(q)}[ (\mathbf{X}_{u,q}- \hat{\mathbf{X}}_{u,q})\frac{\partial \hat{\mathbf{X}}_{u,q}}{\partial \mathbf{q}}] - \alpha\lambda \mathbf{q} \\\nonumber
 \mathbf{b}_{u} =  \mathbf{b}_{u} - \alpha \frac{\partial L}{\partial \mathbf{b}_{u}}=  \mathbf{b}_{u} +  \alpha \sum_{q\in R(u)}[ (\mathbf{X}_{u,q}- \hat{\mathbf{X}}_{u,q})\frac{\partial \hat{\mathbf{X}}_{u,q}}{\partial \mathbf{b}_u}]  \\\nonumber
\mathbf{\theta} =  \mathbf{\theta} - \alpha \frac{\partial L}{\partial \mathbf{\theta}}=  \mathbf{\theta} +  \alpha \sum_{u,q}[ (\mathbf{X}_{u,q}- \hat{\mathbf{X}}_{u,q})\frac{\partial \hat{\mathbf{X}}_{u,q}}{\partial \mathbf{\theta}}] 
\end{eqnarray}
In updating we use
 \begin{equation}\label{equ:derivative}
 \frac{\partial \hat{\mathbf{X}}_{u,q}}{\partial \Theta}=\left\{\begin{matrix}
 [(q_k) \exp\theta +\sum_{k'\neq k}([\mathbf{q}_{k'}-b_{u,k'}]_+)][\frac{\exp u_k}{\sum_{k}\exp u_k}-\frac{\exp^2 u_k}{(\sum_k \exp u_k)^2}] & if \Theta=u_k \\ 
\frac{\exp u_k \exp \theta +\sum_{k'\neq k} \exp u_{k'}}{\sum_k \exp u_k}  & if \Theta= q_{k} \& q_{k} > b_{u,k} \\ 
\frac{\exp u_k \exp \theta }{\sum_k \exp u_k}  & if \Theta= q_{k} \& q_{k} \leq b_{u,k} \\ 
-(1-\frac{\exp\mathbf{u}_k}{\sum_{k'}\exp\mathbf{u}_{k'}}) & if \Theta = b_{u,k} \& q_k > b_{u,k} \\
0 & if \Theta = b_{u,k} \& q_k \leq b_{u,k} \\
\sum_k  \frac{q_k \exp (u_k)}{\sum_{k'} \exp (u_{k'})} \exp \theta & if \Theta = \theta 
\end{matrix}\right.
\end{equation}

%chensi
If we use a ``softer'' cut-off constraint for  non-prominent aspect. We have the prediction according to 
\begin{equation}\label{equ:MF-NCR}
 \hat{\mathbf{X}}_{u,q}=\sum_{k=1}^{K} \frac{\exp \mathbf{u}_k}{\sum_{k'} \exp \mathbf{u}_{k'}} [ \exp\theta \mathbf{q}_k  + \sum_{k'\neq k} [\mathbf{q}_{k'}-b_{u,k'}] ].
\end{equation}

The interpretation is that (1) the evaluation of $u$ on $q$ is mainly based on the prominent aspect $k$ (2) the user $u$ sets a cut-off point for non prominent aspects $k'$. The item performance on non-prominent aspect contributes a term of $[\mathbf{q}_{k'}-b_{u,k'}] $ to the overall evaluation.
 
Thus \begin{eqnarray}
 \mathbf{u}= \mathbf{u} - \alpha \frac{\partial L}{\partial \mathbf{u}} =  \mathbf{u} +  \alpha \sum_{q\in R(u)}[ (\mathbf{X}_{u,q}- \hat{\mathbf{X}}_{u,q})\frac{\partial \hat{\mathbf{X}}_{u,q}}{\partial \mathbf{u}}] - \alpha\lambda \mathbf{u} \\\nonumber
 \mathbf{q} =  \mathbf{q} - \alpha \frac{\partial L}{\partial \mathbf{q}}=  \mathbf{q} +  \alpha \sum_{u\in R(q)}[ (\mathbf{X}_{u,q}- \hat{\mathbf{X}}_{u,q})\frac{\partial \hat{\mathbf{X}}_{u,q}}{\partial \mathbf{q}}] - \alpha\lambda \mathbf{q} \\\nonumber
 \mathbf{b}_{u} =  \mathbf{b}_{u} - \alpha \frac{\partial L}{\partial \mathbf{b}_{u}}=  \mathbf{b}_{u} +  \alpha \sum_{q\in R(u)}[ (\mathbf{X}_{u,q}- \hat{\mathbf{X}}_{u,q})\frac{\partial \hat{\mathbf{X}}_{u,q}}{\partial \mathbf{b}_u}]  \\\nonumber
\mathbf{\theta} =  \mathbf{\theta} - \alpha \frac{\partial L}{\partial \mathbf{\theta}}=  \mathbf{\theta} +  \alpha \sum_{u,q}[ (\mathbf{X}_{u,q}- \hat{\mathbf{X}}_{u,q})\frac{\partial \hat{\mathbf{X}}_{u,q}}{\partial \mathbf{\theta}}] 
\end{eqnarray}
In updating we use
 \begin{equation}\label{equ:derivative}
 \frac{\partial \hat{\mathbf{X}}_{u,q}}{\partial \Theta}=\left\{\begin{matrix}
 [(q_k) \exp\theta +\sum_{k'\neq k}([\mathbf{q}_{k'}-b_{u,k'}]_+)][\frac{\exp u_k}{\sum_{k}\exp u_k}-\frac{\exp^2 u_k}{(\sum_k \exp u_k)^2}] & if \Theta=u_k \\ 
\frac{\exp u_k \exp \theta +\sum_{k'\neq k} \exp u_{k'}}{\sum_k \exp u_k}  & if \Theta= q_{k}  \\ 
-(1-\frac{\exp\mathbf{u}_k}{\sum_{k'}\exp\mathbf{u}_{k'}}) & if \Theta = b_{u,k}  \\
\sum_k  \frac{q_k \exp (u_k)}{\sum_{k'} \exp (u_{k'})} \exp \theta & if \Theta = \theta 
\end{matrix}\right.
\end{equation}

\section{The Beta model}
%motivations
 We are motivated to design this model by the following assumptions: (1) we allow  multiple ($0~K$) aspects in a session to be prominent. (2) We use positive and negative observations, e.g. a click is negative and a buy is positive. (3) For prominent aspects, we directly model the likelihood by the Beta function. (4) For non-prominent aspects, we assume that they do not contribute to the likelihood of being positive or negative, ie. $p(o=1|g_k=0)=0.5$.  (5) We assume that the final label is assigned by a very strict measurement, i.e. the item must be ``good'' on all aspects.
 
 %experiments
As such, in experiments, we should monitor model performances on the following issues. (1) A proper baseline would be to also treat the recommendation system as binary classification problems. For simplicity, I would recommend starting with those traditional MF or classifiers without any number, or time related features. (2) The binary classification framework is easily extend to one class classification scheme for implicit feedback. However, I would recommend to first testify the performance for two classes. When we involve implicit feedback, a fair comparison is important. It would be best if we could verify the capacity of our model on two-class problems, before we use confidence levels or sampling techniques for the negative implicit feedback. (3) Shall we induce sparse constraints on the number of prominent aspects? This could be done by choosing proper values for $a,b$.  (4) Tuning the parameter $K$ is the last step in experimental studies.
 
 %future extensions
For future extensions, we should keep in mind that this model might suffer from the following weaknesses. (1) The likelihood is obtained by a multiplication of all aspects. Although this is proved by preceding pilot studies, that a complete BTL models multiplying all aspects is better than a simplified BTL model on just one aspect, I am not very confident about the conclusion. We can easily modify the model to account for an opposite assumption, that the ``good'' performance on one aspect can override other aspects.  (2) The non-prominent aspects are discarded. This could be fixed by introducing a step function, i.e. $p(o_v=1|g_k=0)=f(v-\theta)$.  (3)  Shall we consider ratings (normalized to $(0,1)$), instead of positive and negative samples? A fair comparative study with MF is needed to answer this question.  If the answer is, we can modify the generation of observations. All the above directions are possible and can be implemented without much difficulty.
 
\begin{figure}
  \centering
  \tikz{ %
%users and items
    \node[const] (a) {$a$} ; %
    \node[const, right=2 of a] (b) {$b$} ; 
    \node[const, right =2 of b] (alpha) {$\alpha$};
       \node[const, right =2 of alpha] (beta) {$\beta$};
     \node[latent, below =of a] (u1) {$u_1$};
     \node[const, right = of u1] (dots) {$\cdots $};
        \node[latent, right = 2 of u1 ] (uK) {$u_K$};
        
  	\node[latent, right = of uK] (v1) {$v_1$};
	     \node[const, right = of v1] (vdots) {$\cdots $};
        \node[latent, right = 3 of v1 ] (vK) {$v_K$};
        
       	\edge{a}{u1};
	\edge{b}{u1};
	\edge{a}{uK};
	\edge{b}{uK};
	
	 \plate[inner sep=0.1cm, xshift=-0cm, yshift=0.12 cm] {U} {(u1) (dots) (uK) } {M}; 
	 \edge{alpha}{v1};
	\edge{alpha}{vK};
	\edge{beta}{v1};
	\edge{beta}{vK};
       \plate[inner sep=0.1cm, xshift=-0cm, yshift=0.12 cm] {V} {(v1) (vdots) (vK) } {N}; 
       %per session
       
     \node[latent, below = 2 of u1] (g1) {$g_1$};    
     	     \node[const, right = of g1] (gdots) {$\cdots $};
        \node[latent, right = 2 of g1 ] (gK) {$g_K$};
           
           	  \node[latent,  below = of g1] (h1) {$h_1$};    
                	     \node[const, right = of h1] (hdots) {$\cdots $};
        \node[latent, right = 2 of h1 ] (hK) {$h_K$};
           \node[obs, below =  of hdots] (o) {$o$};  
           
	 
	
	  \edge{u1}{g1};
	    \edge{uK}{gK};
	    
		  \edge{v1}{h1};
	  	  \edge{g1}{h1};
		  
		  	  \edge{vK}{hK};
	  	  \edge{gK}{hK};
		  
	  \edge{h1}{o};
	   \edge{hdots}{o};
	    \edge{hK}{o};
	    
	

	    \plate[inner sep=0.1cm, xshift=0cm, yshift=0.12 cm] {S} {(o) (h1) (hdots) (hK) } {L}; 
	    \plate[inner sep=0.4 cm, xshift=-0.12cm, yshift=0.12 cm] {D} {(g1) (gdots) (gK) (S) } {S}; 
 }
 \caption{Plate notation of the proposed sigmoidal model}\label{fig:sigmoidal}
\end{figure}

First we generate the user preferences and item features. 
\begin{itemize}
\item For aspect $k=1: K$, for user $u=1: M$, sample user preference on aspect $k: u_k\sim Beta(a,b)$. Unlike most of previous research, the user preference vector $u$ in our work is not the normalized weight over all aspects.
\item For items $v=1: N$, sample item features for the item universe $v_k \sim Beta (\alpha,\beta)$. Again, unlike most of previous research, the user preference vector $u$ in our work is not the normalized weight over all aspects.

\end{itemize}

Next, given $S$ sessions of user $u$
\begin{itemize}
\item For each session $s$, for aspect $k=1:K$, generate aspect indicator $g_{s,k} \sim Bern(u_k)$. If $g_{s,k}=1$, then $k$ is a prominent aspect for user $u$ in the session $s$.
\item For each item $v$ within the session $s$ of length $L$
\begin{itemize}
\item For each aspect $k$, generate the indicator $p(h_{s,v,k}|g_{s,k},v_k)=[v_k^{h_{s,v,k}} (1-v_k)^{1-h_{s,v,k}}]^{g_{s,k}} [\frac{1}{2}^{h_{s,v,k}} \frac{1}{2}^{1-h_{s,v,k}}]^{1-g_{s,k}}$
%no sigmoiidal $p(h_{s,v,k}=1|v_k,g_k)=\frac{1}{1+\exp{v_k}}^{g_{s,k}}\frac{1}{2}^{1-g_{s,k}}$
\item For each observed rating $o_v$, generate label observation $o_{s,v}=1-\prod_k (1-h_{s,v,k})$ where $o_v=1$ if and only if $\forall k, h_{s,v,k}=1$
\end{itemize}
\end{itemize}

The joint probability is given by
\begin{equation*}
    p(U,V,G,H,D |a,b,\Lambda) =    \prod_k \prod_u p(u_{k}|a,b)  \prod_{s} p(g_{s,k} | u_k)  \prod_{v} \{ p(v_k|0,\alpha,\beta) p(o_{s,v}|H)  p(h_{s,v,k}|g_{s,k},v_k)\}
    \end{equation*}

%Sigmoidal 
%where the upperbound for $p(h_{s,vk}|g_{s,k},v_k)$is
%\begin{eqnarray*}
%p(h_{s,v,k}|g_{s,k},v_k) &=& \frac{1}{2}^{1-g_{s,k} }[\sigma(v_k)^{h_{s,v,k}} (1-\sigma(v_k))^{1-h_{s,v,k}} ]^{g_{s,k}}\\
%&\leq & \frac{1}{2}^{1-g_{s,k} } \exp(v_k h_{s,v,k}) \sigma(\xi) \exp[ -\frac{v_k+\xi}{2} - \lambda(\xi) (v_k^2- \xi^2) ]
%\end{eqnarray*}

We should notice that $o_{v,k}=1$ indicates $\forall k, h_{v,k}=1$. Thus the likelihood must be decomposed to a term with observations and a term with hidden variables. We have:

\begin{eqnarray}\label{equ:completelikelihoodsigmodal}
\ln p(U,V,G,H,D|a,b,\Lambda,\xi)  & = &\sum_s\sum_{o_{s,v}=1} \sum_k p(h_{s,v,k}=1|g_{s,k},v_k) \\\nonumber
& &+\sum_s \sum_{o_{s,v}=0} p(o_{s,v}=0|H) \sum_k p(h_{s,v,k}|g_{s,k},v_k)\\\nonumber
& &+\sum_u\sum_k p(u_{k}|a,b) + \sum_v \sum_k p(v_k|\alpha,\beta) + \sum_u \sum_s \sum_k p(g_{s,k}|u_k) 
\end{eqnarray}

By variational inference, we have factorize the posterior probability $p(U,V,G,H|D,a,b,\alpha,\beta )=q(U)q(V)q(G)q(H)$. We can see from the following derivations that $q(u_k)\sim Bern(a',b')$
\begin{eqnarray*}
\ln q(u_k) & = & \mathbb{E}_{G,V,H} [\ln p(u_k|a,b)+\sum_s \ln p(g_{s,k}|u_k)]+const\\
&=&(a-1)\ln u_k +(b-1) \ln (1-u_k) + \sum_{s} \mathbb{E}[g_{s,k}] \ln u_k + \sum_s (1-\mathbb{E}[g_{s,k}] \ln (1-u_k) + const \\
\mathbb{E}[u_k] &=& \frac{a+\sum_{s} \mathbb{E}[g_{s,k}] }{a+b+ |S|  }
\end{eqnarray*}

%Sigmoidal: Because $p(h_{s,v,k}=1|g_{s,k},v_k) = \frac{1}{2}^{1-g_{s,k}} \sigma(v_k) g_{s,k} $, $\ln p(h_{s,v,k}=1|g_{s,k},v_k) \geq g_{s,k} [\frac{v_k-\xi}{2} -\lambda(\xi) (v_k^2-\xi^2)]$
Because $p(h_{s,v,k}|g_{s,k},v_k)=[v_k^{h_{s,v,k}} (1-v_k)^{1-h_{s,v,k}}]^{g_{s,k}} [\frac{1}{2}^{h_{s,v,k}} \frac{1}{2}^{1-h_{s,v,k}}]^{1-g_{s,k}}$, we have $\ln p(h_{s,v,k}|g_{s,k},v_k)= g_{s,k} [h_{s,v,k}\ln v_k + (1-h_{s,v,k}\ln (1-v_k)] + (1-g_{s,k}\ln\frac{1}{2}$
%Sigmoidal
%\begin{eqnarray*}
%\ln q(v) & = & \mathbb{E}_{G,U,H} [\ln p(H|v,G)+\ln(v|\alpha,\beta) ] + const\\
%&=& \sum_u \sum_s \sum_k \{ \mathbb{E}[g_{s,k}] [v_kh_{s,k} -\frac{v_k+\xi}{2} -\lambda(\xi) (v_k^2-\xi^2) ] -\frac{1}{2} v^T\Lambda v\} + const\\
%&=& \sum_s\sum_{o_{s,v}=1} \sum_k \mathbb{E} [g_{s,k}] [\frac{v_k-\xi}{2} -\lambda(\xi) (v_k^2-\xi^2)] + 
%\end{eqnarray*}

\begin{eqnarray*}
\ln q(v) & = & \mathbb{E}_{G,U,H} [\ln p(H|v,G)+\ln(v|\alpha,\beta) ] + const\\
&=& \sum_s\sum_{o_{s,v}=1} \sum_k p(h_{s,v,k}=1|g_{s,k},v_k) + \sum_s\sum_{o_{s,v}=0} \sum_k p(h_{s,v,k}|g_{s,k},v_k) +  \sum_v \sum_k p(v_k|\alpha,\beta) \\
\ln q(v_k) & = & \{\sum_s\sum_{o_{s,v}=1}  \mathbb{E}[g_{s,k}] + \sum_s\sum_{o_{s,v}=0}  \Ep[g_{s,k}]\Ep[h_{s,v,k}]\} \ln v_k \\
& & + \sum_s\sum_{o_{s,v}=0} \Ep[g_{s,k}]\Ep[1-h_{s,v,k}] \ln (1-v_k) +  (\alpha-1)\ln v_k + (\beta-1) \ln (1-v_k) + const \\
\Ep[v_k]& = & \frac{\sum_s\sum_{o_{s,v}=1}  \mathbb{E}[g_{s,k}] + \sum_s\sum_{o_{s,v}=0}  \Ep[g_{s,k}]\Ep[h_{s,v,k}]+\alpha}{\sum_s\sum_{o_{s,v}=1}  \mathbb{E}[g_{s,k}] + \sum_s\sum_{o_{s,v}=0}  \Ep[g_{s,k}]\Ep[h_{s,v,k}]+\alpha+\beta + \sum_s\sum_{o_{s,v}=0} \Ep[g_{s,k}]\Ep[1-h_{s,v,k}]}
\end{eqnarray*}



\begin{eqnarray*}
\ln q(G) & = & \mathbb{E}_{U,V,H} (\ln p(G|U)+\ln (H|G,U))+ const \\
\ln q(g_{s,k}) & = & \ln p(g_{s,k}|u_k) + \sum_{o_{s,v}=1}  p(h_{s,v,k}=1|g_{s,k},v_k) + \sum_{o_{s,v}=0}  p(h_{s,v,k}|g_{s,k},v_k) \\
&=& g_{s,k} \{\Ep[\ln u_k] + \Ep[\ln v_k] + \sum_{o_{s,v}=1}  \Ep[\ln v_k] + \sum_{o_{s,v}=0} [ \Ep[h_{s,v,k}] \Ep[\ln v_k] + \Ep[1-h_{s,v,k} ] \Ep[1-\ln v_k] ] \}\\
& & + (1-g_{s,k}) \{\Ep[\ln (1-u_k)] + \sum_{o_{s,v} } \ln \frac{1}{2}  \}  \\
\mathbb{E}[g_{s,k}]&=&\frac{\Ep[\ln u_k] + \Ep[\ln v_k] + \sum_{o_{s,v}=1}  \Ep[\ln v_k] + \sum_{o_{s,v}=0} [ \Ep[h_{s,v,k}] \Ep[\ln v_k] + \Ep[1-h_{s,v,k} ] \Ep[1-\ln v_k] ] }{\Ep[\ln u_k] + \Ep[\ln v_k] + \sum_{o_{s,v}=1}  \Ep[\ln v_k] + \sum_{o_{s,v}=0} [ \Ep[h_{s,v,k}] \Ep[\ln v_k] + \Ep[1-h_{s,v,k} ] \Ep[1-\ln v_k] ]  +(1-g_{s,k}) \{\Ep[\ln (1-u_k)] + \sum_{o_{s,v} } \ln \frac{1}{2}  \}  }
\end{eqnarray*}

where $\Ep[\ln(u_k) = \phi(a')\phi(a'+b'), \Ep[\ln(v_k) = \phi(\alpha')\phi(\beta')$.  

\begin{eqnarray*}
\ln q(H) & = & \mathbb{E}_{G,U,V} (\ln p(H|V,G) + \ln p(O|H)) \\
\ln q(h_{s,v,k}=1) &=& \sum_{o_{s,v}=1} \sum_k p(h_{s,v,k}=1|g_{s,k},v_k) +\sum_{o_{s,v}=0} \sum_{h_{s,v,k'}} p(o_{s,v}=0|H)  p(h_{s,v,k'}|g_{s,k'},v_k) + const\\
\Ep [h_{s,v,k} ] &= &   q(h_{s,v,k}=1) + q(h_{s,v,k}=0) 
\end{eqnarray*}



\end{document}